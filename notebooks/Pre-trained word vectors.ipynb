{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/oscon.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we show how to take [GloVe](http://nlp.stanford.edu/projects/glove/) word vectors trained based on Common Crawl web data, and incorporate them into a our text classification.\n",
    "\n",
    "Start by setting the path to which you downloaded the pre-trained vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.environ[\"HOME\"], \"data\") if not 'win' in sys.platform else \"c:\\\\tmp\\\\data\"\n",
    "path_to_glove = os.path.join(DATA_DIR,\"glove.840B.300d.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "digit_to_word_map[0]=\"PAD_TOKEN\"\n",
    "times_steps = 6\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,times_steps+1))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),\n",
    "                                     rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),\n",
    "                                      rand_seq_len)\n",
    "\n",
    "    #Padding\n",
    "    if rand_seq_len<times_steps:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(times_steps-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                   [0]*(times_steps-rand_seq_len))\n",
    "\n",
    "    even_sentences+=[\" \".join([digit_to_word_map[r] for\n",
    "                               r in rand_even_ints])]\n",
    "    odd_sentences+=[\" \".join([digit_to_word_map[r] for\n",
    "                              r in rand_odd_ints])] \n",
    "\n",
    "data = even_sentences+odd_sentences\n",
    "seqlens = seqlens + seqlens\n",
    "#Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "#Inverse map    \n",
    "index2word_map = dict([(index,word) for word,index in\n",
    "                       word2index_map.items()])            \n",
    "vocabulary_size = len(index2word_map)\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "\n",
    "\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "def get_sentence_batch(batch_size,data_x,\n",
    "                       data_y,data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()]\n",
    "         for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are 2.2 million words in the vocabulary of the pre-trained GloVe embeddings we downloaded â€” we only take the GloVe vectors for words that appear in our own, tiny vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def get_glove(path_to_glove,word2index_map):\n",
    "        \n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0 \n",
    "    with zipfile.ZipFile(path_to_glove) as z:\n",
    "        with z.open(\"glove.840B.300d.txt\") as f:\n",
    "            for line in f:\n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\")) \n",
    "                if word in word2index_map:\n",
    "                    print(word)\n",
    "                    count_all_words+=1\n",
    "                    coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                    coefs/=np.linalg.norm(coefs) \n",
    "                    embedding_weights[word] = coefs\n",
    "                if count_all_words==len(word2index_map)-1:\n",
    "                    break\n",
    "    return embedding_weights \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our function is a dictionary, mapping from each word to its vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2embedding_dict = get_glove(path_to_glove,word2index_map)\n",
    "print(word2embedding_dict[\"one\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place these vectors in a matrix, where each row index corresponds to the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_SIZE = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2index_map),GLOVE_SIZE))\n",
    "\n",
    "for word,index in word2index_map.items():\n",
    "    if not word == \"pad_token\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the \"pad_token\" word, we set the corresponding vector to 0. The padded vectors are ignored by dynamic_rnn(), as we saw above.\n",
    "\n",
    "Now, back to TensorFlow. First, we create the exact same placeholders we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size=128;num_classes = 2\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an embedding_placeholder, to which we feed the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size,\n",
    "                                                    GLOVE_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our embeddings are initialized with the content of embedding_placeholder, using the assign function to assign initial values to the embeddings variable.   We set trainable=True, to tell TensorFlow we want to update the values of the word vectors, by optimizing them for the task at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED = True\n",
    "\n",
    "if PRE_TRAINED:\n",
    "    with tf.device('/cpu:0'):\n",
    "            embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, GLOVE_SIZE]),\n",
    "                                     trainable=True)\n",
    "            embedding_init = embeddings.assign(embedding_placeholder)\n",
    "            embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "\n",
    "else:\n",
    "    with tf.device('/cpu:0'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size,\n",
    "                                   embedding_dimension],\n",
    "                                  -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we repeat the exact same LSTM network structure we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 32\n",
    "with tf.variable_scope(\"lstm\"):\n",
    " \n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(hidden_layer_size)\n",
    "    _, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)\n",
    "last_state = states[1]\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([hidden_layer_size,num_classes],mean=0,stddev=.01))\n",
    "b = tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n",
    "\n",
    "#extract the final state and use in a linear layer\n",
    "final_pred = tf.matmul(last_state,W) + b\n",
    "#\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits  = final_pred,labels = _labels)                         \n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),\n",
    "                              tf.argmax(final_pred,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                   tf.float32)))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, we are ready to train. We initialize the embedding_placeholder by feeding it our embedding_matrix. Importantly, we do so after calling tf.global_variables_initializer() â€” doing so in the reverse order, would overrun the pre-trained vectors with a default initializer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(embedding_init, feed_dict=\n",
    "         {embedding_placeholder: embedding_matrix})\n",
    "for step in range(1000):\n",
    "    x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                                       train_x,train_y,\n",
    "                                                       train_seqlens)\n",
    "    sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                   _seqlens:seqlen_batch})\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                           _labels:y_batch,\n",
    "                                           _seqlens:seqlen_batch})\n",
    "        print(\"Accuracy at %d: %.5f\" % (step, acc)) \n",
    "\n",
    "for test_batch in range(5):\n",
    "    x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                    test_x,test_y,\n",
    "                                                    test_seqlens)\n",
    "    batch_pred,batch_acc = sess.run([tf.argmax(final_pred,1),\n",
    "                                     accuracy],\n",
    "                                    feed_dict={_inputs:x_test,\n",
    "                                               _labels:y_test,\n",
    "                                               _seqlens:seqlen_test})\n",
    "    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))   \n",
    "    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick hands-on\n",
    "* Extract the trained embedding vectors\n",
    "* Choose a word (from \"one\" to \"nine\") and find its nearest neighbors -- sort all the other word vectors by their distance to the target word. Use numpy or sklearn.\n",
    "* Re-run the network, this time without training the word vectors beyond their initial values from GloVe. Do you get different nearest neighbors?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
