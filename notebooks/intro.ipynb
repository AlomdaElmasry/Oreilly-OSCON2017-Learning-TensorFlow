{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0c9934c-39fa-46a8-9d06-611a3c5c4645"
    }
   },
   "source": [
    "<img src=\"./img/oscon.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Install Checks \n",
    "The purpose of this section is to make sure you have all the modules we will use throughout the training. \n",
    "\n",
    "--\n",
    "\n",
    "If you haven't already done so, we recommend that you install TensorFlow in a seperate virtualenv (or conda environment). First create a virtualenv (pip install virtualenv, if you don't have the software installed already). \n",
    "\n",
    "~~~bash\n",
    "$cd ~\n",
    "$mkdir envs\n",
    "$virtualenv ~/envs/tensorflow\n",
    "~~~\n",
    "\n",
    "Then activate the virtualenv and install TensorFlow (this will be the basic CPU-enabled version, but that's good enough for now).  \n",
    "\n",
    "~~~bash\n",
    "$source ~/envs/tensorflow/bin/activate\n",
    "(tensorflow)$pip install tensorflow\n",
    "~~~\n",
    "\n",
    "Additoinal resources: \n",
    "- https://www.tensorflow.org/install/install_mac\n",
    "- https://www.tensorflow.org/install/install_windows\n",
    "- https://www.tensorflow.org/install/install_linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note what version of TensorFlow you are currently using. We assume you are using a version $\\geq$ 1.0, some (rather small) adaptation might be necessary if you are using a previous version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import some other modules that will be useful throughout the training. Most of these are come with the Anaconda package (but pip install keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World\n",
    "We kick off with a simple \"Hello World\" exmaple. While simple and intuitive, this already contains many of the TensorFlow elements we will discuss in depth later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = tf.constant(\"Hello\")\n",
    "w = tf.constant(\" World!\")\n",
    "hw = h + w\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ans = sess.run(hw)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(h)\n",
    "print(hw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A first (and simple) TensorFlow classifier \n",
    "# MNIST \n",
    "The MNIST (Mixed National Institue of Standards and Technology) handwritten digits dataset is one of the most researched datasets in image processing and machine learning, and has played an important role in the development of artificial neural networks.\n",
    "\n",
    "Our first example will be a simple softmax-regression model on the raw pixels of the digit images. First, let's take a look at the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import sys\n",
    "\n",
    "# This is where the MNIST data will be downloaded to. If you already have it on your \n",
    "# machine then set the path accordingly to prevent an extra download. \n",
    "DATA_DIR = '/tmp/data' if not 'win' in sys.platform else \"c:\\\\tmp\\\\data\"\n",
    "\n",
    "# Load data \n",
    "data = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "print(\"Nubmer of training-set images: {}\".format(len(data.train.images)))\n",
    "print(\"Luckily, there are also {} matching labels.\".format(len(data.train.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset loaded we can plot a few examples.\n",
    "\n",
    "We will start with single images and then plot a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline  \n",
    "\n",
    "IMAGE_IX_IN_DATASET = 0\n",
    "\n",
    "first_img = data.train.images[IMAGE_IX_IN_DATASET].reshape(28, 28)\n",
    "first_lbl = data.train.labels[IMAGE_IX_IN_DATASET].argmax()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(first_img, cmap='gray')\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"This is supposed to be a {}\".format(first_lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot an entire batch of digits, we stack them both horizontally and vertically, formly a 10X10 grid of digits. \n",
    "\n",
    "We then plot the image <b> negative </b> for better visibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 10\n",
    "img = np.vstack([np.hstack([img.reshape(28, 28) \n",
    "                            for img in data.train.images[np.random.choice(1000, N_IMAGES)]])\n",
    "                 for i in range(N_IMAGES)])\n",
    "img = np.logical_not(img)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"100 random digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "Put simply, the softmax regression model will figure out for each pixel in the image, which digits tend to have high (or low) values in that location. For instance, the center of the image will tend to be white for zeros, but black for sixes. Thus, a black pixel in the center of an image will be evidence against the image containing a zero, and in favor of it containing a six.\n",
    "\n",
    "Learning in this model consists of finding weights that tell us how to accumulate evidence for the existence of each of the digits. With softmax regression, we will not use the spatial information in the pixel layout in the image.\n",
    "\n",
    "$ evidence(i) = \\langle x, w_i \\rangle \\ \\  [+b_i]$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the raw (vector) image and labael -- see what it looks like when not a rectangle\n",
    "img = data.train.images[:1000]\n",
    "\n",
    "# Plot \n",
    "plt.figure()\n",
    "plt.imshow(np.logical_not(img).T, cmap='gray')\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"Each column is an image unrolled...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it might look like there is no signal here. We will do two things:\n",
    "    1. cut out only the center of the digits image (the rest is white-space)\n",
    "    2. soft the columns by the actual digit represented in the image \n",
    "    \n",
    "Once we do this, we will see that there is actually a clear structure in the individual pixels. While the human eye is not able to recognize the digits (maybe with some practice...), this is enough for the SoftMax model to go on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "N_IMAGES = 1000\n",
    "\n",
    "# Cut out the center part of the image (Actual digit)\n",
    "center_img = [img.reshape(28, 28)[8:22, 8:22].ravel() \n",
    "              for img in  data.train.images[:N_IMAGES]]\n",
    "\n",
    "# Sort by digits\n",
    "sorted_lbls = np.argsort(data.train.labels.argmax(axis=1)[:N_IMAGES])\n",
    "center_img = np.array(center_img)[sorted_lbls]\n",
    "\n",
    "# Plot \n",
    "plt.figure()\n",
    "plt.imshow(np.logical_not(center_img).T, cmap='gray')\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"Each column is the center of an image, unrolled...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we formulate the actual <b> model </b>.\n",
    "\n",
    "There are a few elements to notice here:\n",
    "    1. placehodler\n",
    "    2. Variable\n",
    "    3. Dimensions; as in [None, 784]\n",
    "    4. matmul\n",
    "    \n",
    "We will discuss all of these in great detail later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We start by building the model \n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "y_pred = tf.matmul(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next element we need is the <b> objective </b> (a.k.a loss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = \\\n",
    "    tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the model -- the W matrix are optimized using Gradient Descent. This method essentially consists of taking many small steps in the right direction. But more about that later... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final elements we need are measures of how well our model is preforming.\n",
    "    1. correct_mask: an indicator per sample whether or not it is labeled correctly\n",
    "    2. accuracy: the fraction of samples labeled correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the necessary elements in place and we are ready to run the model. Recall that in TensorFlow we first define a computation graph and then run it. The next part deals with running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 1000\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Train\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(NUM_STEPS):\n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "\n",
    "    # Test\n",
    "    is_correct, acc = sess.run([correct_mask, accuracy], \n",
    "                               feed_dict={x: data.test.images, y_true: data.test.labels})\n",
    "\n",
    "print(\"Accuracy: {:.4}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which images did we get wrong?\n",
    "The softmax-regression model gets approx. 8% of the test-set wrong! Later on when we use CNNs things will look much better... (State of the art systems acheive virtually perfect accuracy on this dataset).\n",
    "\n",
    "But first let's take a look at which digit images we got right, and which we got wrong. Recall softmax-regression doesn't model the relationship between pixels, but rather considers them independently. Does this explain some of the errors we see?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_ix = np.where(is_correct)[0]\n",
    "correct_img = data.train.images[correct_ix]\n",
    "\n",
    "N_IMAGES = 10\n",
    "img = np.vstack([np.hstack([img.reshape(28, 28) \n",
    "                            for img in correct_img[np.random.choice(len(correct_ix), N_IMAGES)]])\n",
    "                 for i in range(N_IMAGES)])\n",
    "img = np.logical_not(img)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"We got this bunch right!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incorrect_ix = np.where(np.logical_not(is_correct))[0]\n",
    "incorrect_img = data.train.images[incorrect_ix]\n",
    "\n",
    "N_IMAGES = 10\n",
    "img = np.vstack([np.hstack([img.reshape(28, 28) \n",
    "                            for img in incorrect_img[np.random.choice(len(incorrect_ix), N_IMAGES)]])\n",
    "                 for i in range(N_IMAGES)])\n",
    "img = np.logical_not(img)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"...but didn't do so well with these ones!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## 1. Confusion matrix \n",
    "A confusion matrix is a table where the [i ,j]-th element details the number of examples from class i which were classified as class j. The diagonal reprsents correct classification, and other elements represent the types of error that occur. \n",
    "\n",
    "### 1.1 Construct and print the confusion matrix for the softmax regression model and the MNIST dataset. \n",
    "\n",
    "### 1.2 Which digits are most confused with which? Is it symmetric? Does this make sense?\n",
    "\n",
    "\n",
    "useful hints:\n",
    "1. \n",
    "~~~python \n",
    "from sklearn.metrics import confusion_matrix \n",
    "~~~\n",
    "2. \n",
    "~~~python\n",
    "sess.run([y_true, y_pred ]... \n",
    "~~~\n",
    "\n",
    "## 2. Softmax regression with a bias term\n",
    "It is common to add a scalar \"bias\" term to the regression formulation:\n",
    "\n",
    "$ evidence(i) = \\langle x, w_i \\rangle + b_i $ \n",
    "\n",
    "### 2.1 Add a bias term to the TF model\n",
    "\n",
    "### 2.2 Did the results chnage? Explain...\n",
    "\n",
    "---\n",
    "Solutions can be loaded in the next Cells.\n",
    "\n",
    "Also available at: https://github.com/Hezi-Resheff/Oreilly-OSCON2017-Learning-TensorFlow/tree/master/solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../solutions/intro_ex1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../solutions/intro_ex2.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "64fc7e81-424c-4384-bd40-efdb2cf5ec2e": {
     "id": "64fc7e81-424c-4384-bd40-efdb2cf5ec2e",
     "prev": null,
     "regions": {
      "cad98b11-ae68-44c7-bc2d-7d2cf3988a2c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0c9934c-39fa-46a8-9d06-611a3c5c4645",
        "part": "whole"
       },
       "id": "cad98b11-ae68-44c7-bc2d-7d2cf3988a2c"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
